{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066ce4c8-6cfa-4c36-8459-cad290104f32",
   "metadata": {},
   "source": [
    "Existen algunas webs que permiten trabajar con scrapy. \n",
    "Es muy importante webs que tengan un **HTML muy limpio**, y en lo posible básico, y selectores CSS muy claros. \n",
    "Suelen ser supermercados, inmobiliarias, ventas de coches de ocasión pero no de grandes corpopraciones. \n",
    "Sobre todo **evitar javascript**\n",
    "\n",
    "Desarrollar un spider Scrapy que recorra un catálogo de productos de un sitio web público y extraiga los siguientes datos, como mínimo:\n",
    "\n",
    "Nombre o título del producto.\n",
    "\n",
    "Precio.\n",
    "\n",
    "Estado o disponibilidad. (si es posible)\n",
    "\n",
    "Categoría o etiqueta (si existe).\n",
    "\n",
    "Cualquier otro campo que consideres oportuno. \n",
    "\n",
    "El spider debe ser capaz de recorrer varias páginas mediante la paginación del sitio.\n",
    "\n",
    "Los datos extraídos se almacenarán en un fichero .json o .csv para su posterior análisis en pandas.\n",
    "En este punto investiga 4 o 5 funciones de Pandas que no hayamos visto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d04c1453-4352-4e83-a6c6-420adae190d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'mi_catalogo_chimbita', using template directory '/opt/conda/lib/python3.11/site-packages/scrapy/templates/project', created in:\n",
      "    /home/jovyan/work/UD1/mi_catalogo_chimbita\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd mi_catalogo_chimbita\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject mi_catalogo_chimbita "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c478347-c75c-4f0c-9c6e-85a965ff0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/UD1/mi_catalogo_chimbita\n"
     ]
    }
   ],
   "source": [
    "cd mi_catalogo_chimbita "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa9e47c-baec-41f8-b0ef-edb850375ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'libros' using template 'basic' in module:\n",
      "  mi_catalogo_chimbita.spiders.libros\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider libros books.toscrape.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa7140c-84a4-4903-b4df-0105a703ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class LibrosSpider(scrapy.Spider):\n",
    "    name = \"libros\"\n",
    "    start_urls = [\"http://books.toscrape.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for libro in response.css('article.product_pod'):\n",
    "            titulo = libro.css('h3 a::attr(title)').get()\n",
    "            precio = libro.css('p.price_color::text').get()\n",
    "            # disponibilidad = libro.css('p.instock.availability::text').getall()\n",
    "            # disponibilidad = ''.join(disponibilidad).strip()\n",
    "            # rating_class = libro.css('p.star-rating::attr(class)').get()\n",
    "            # rating = rating_class.replace('star-rating','').strip()\n",
    "            # url = libro.css('h3 a::attr(href)').get()\n",
    "\n",
    "            yield {\n",
    "                'titulo': titulo,\n",
    "                'precio': precio,\n",
    "                # 'disponibilidad': disponibilidad,\n",
    "                # 'rating': rating,\n",
    "                # 'url': url\n",
    "            }\n",
    "\n",
    "     \n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c89cc0-9233-40aa-bc86-0e50d87bd010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 11:57:00 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: mi_catalogo_chimbita)\n",
      "2025-11-12 11:57:00 [scrapy.utils.log] INFO: Versions:\n",
      "{'lxml': '6.0.2',\n",
      " 'libxml2': '2.14.6',\n",
      " 'cssselect': '1.3.0',\n",
      " 'parsel': '1.10.0',\n",
      " 'w3lib': '2.3.1',\n",
      " 'Twisted': '25.5.0',\n",
      " 'Python': '3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) '\n",
      "           '[GCC 12.3.0]',\n",
      " 'pyOpenSSL': '23.2.0 (OpenSSL 3.1.3 19 Sep 2023)',\n",
      " 'cryptography': '41.0.4',\n",
      " 'Platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35'}\n",
      "2025-11-12 11:57:00 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-11-12 11:57:00 [asyncio] DEBUG: Using selector: EpollSelector\n",
      "2025-11-12 11:57:00 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2025-11-12 11:57:00 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
      "2025-11-12 11:57:00 [scrapy.extensions.telnet] INFO: Telnet Password: 000ec1065717cd01\n",
      "2025-11-12 11:57:00 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-11-12 11:57:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'mi_catalogo_chimbita',\n",
      " 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n",
      " 'DOWNLOAD_DELAY': 1,\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'mi_catalogo_chimbita.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['mi_catalogo_chimbita.spiders']}\n",
      "2025-11-12 11:57:00 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-11-12 11:57:00 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
      " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-11-12 11:57:00 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2025-11-12 11:57:00 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-11-12 11:57:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-11-12 11:57:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-11-12 11:57:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
      "2025-11-12 11:57:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com> (referer: None)\n",
      "2025-11-12 11:57:02 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-11-12 11:57:02 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: libros.json\n",
      "2025-11-12 11:57:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 468,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 5805,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 1.802207,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 11, 12, 11, 57, 2, 134736, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 51447,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'items_per_minute': 0.0,\n",
      " 'log_count/DEBUG': 5,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 70496256,\n",
      " 'memusage/startup': 70496256,\n",
      " 'response_received_count': 2,\n",
      " 'responses_per_minute': 120.0,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2025, 11, 12, 11, 57, 0, 332529, tzinfo=datetime.timezone.utc)}\n",
      "2025-11-12 11:57:02 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2025-11-12 11:57:02 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: mi_catalogo_chimbita)\n",
      "2025-11-12 11:57:02 [scrapy.utils.log] INFO: Versions:\n",
      "{'lxml': '6.0.2',\n",
      " 'libxml2': '2.14.6',\n",
      " 'cssselect': '1.3.0',\n",
      " 'parsel': '1.10.0',\n",
      " 'w3lib': '2.3.1',\n",
      " 'Twisted': '25.5.0',\n",
      " 'Python': '3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) '\n",
      "           '[GCC 12.3.0]',\n",
      " 'pyOpenSSL': '23.2.0 (OpenSSL 3.1.3 19 Sep 2023)',\n",
      " 'cryptography': '41.0.4',\n",
      " 'Platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35'}\n",
      "2025-11-12 11:57:02 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-11-12 11:57:02 [asyncio] DEBUG: Using selector: EpollSelector\n",
      "2025-11-12 11:57:02 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2025-11-12 11:57:02 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
      "2025-11-12 11:57:02 [scrapy.extensions.telnet] INFO: Telnet Password: ccc0f3cabf0eab31\n",
      "2025-11-12 11:57:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-11-12 11:57:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'mi_catalogo_chimbita',\n",
      " 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n",
      " 'DOWNLOAD_DELAY': 1,\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'mi_catalogo_chimbita.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['mi_catalogo_chimbita.spiders']}\n",
      "2025-11-12 11:57:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-11-12 11:57:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
      " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-11-12 11:57:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2025-11-12 11:57:02 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-11-12 11:57:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-11-12 11:57:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-11-12 11:57:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
      "2025-11-12 11:57:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com> (referer: None)\n",
      "2025-11-12 11:57:04 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-11-12 11:57:04 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: libros.csv\n",
      "2025-11-12 11:57:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 468,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 5805,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 1.721167,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 11, 12, 11, 57, 4, 666256, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 51447,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'items_per_minute': 0.0,\n",
      " 'log_count/DEBUG': 5,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 70569984,\n",
      " 'memusage/startup': 70569984,\n",
      " 'response_received_count': 2,\n",
      " 'responses_per_minute': 120.0,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2025, 11, 12, 11, 57, 2, 945089, tzinfo=datetime.timezone.utc)}\n",
      "2025-11-12 11:57:04 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl libros -o libros.json\n",
    "!scrapy crawl libros -o libros.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a83dbf-81f3-4329-ad1a-45bfb8213cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530cfa4-cd18-46c1-9f39-aed6ece8768a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
