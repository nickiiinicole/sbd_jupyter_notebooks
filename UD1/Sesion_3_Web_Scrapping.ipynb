{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a474dc78-c7ba-4dcb-baa0-f45d8d2d9e11",
   "metadata": {},
   "source": [
    "# Web Scrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864e90f-72ba-4593-95bb-d3fdd8b1e866",
   "metadata": {},
   "source": [
    "## 1. Beatifulsoap\n",
    "\n",
    "El **_web scraping_** es una técnica que permite **extraer información de sitios web** de forma automatizada.\n",
    "En lugar de copiar datos manualmente desde una página, un programa “visita” esa página, descarga su código HTML, y luego extrae los fragmentos de información que interesan (por ejemplo, títulos, precios, autores, etc.).\n",
    "Se basa en el análisis y extracción de datos de HTML/XML\n",
    "\n",
    "Cuando ingresas una URL en el navegador:\n",
    "\n",
    "* El navegador envía una solicitud HTTP (request) al servidor del sitio.\n",
    "\n",
    "* El servidor responde con el HTML de la página.\n",
    "\n",
    "* El navegador interpreta ese HTML y lo muestra visualmente.\n",
    "\n",
    "El **scraper hace lo mismo**, pero en lugar de mostrar la página, **analiza el HTML y toma los datos** que necesita.\n",
    "\n",
    "Es rápida, fácil de usar pero solo válida para pequeños proyectos\n",
    "\n",
    "En el siguiente ejemplo vemos como Python hace un request a una página\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9798d758-5a88-4ec5-a9cb-8a8bf6b64ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexión exitosa. Página descargada correctamente.\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vect\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL del sitio que queremos scrapear\n",
    "url = \"https://es.wikipedia.org/wiki/Python\"\n",
    "\n",
    "# Encabezados HTTP personalizados\n",
    "# Es importante incluir un User-Agent identificable\n",
    "headers = {\n",
    "    \"User-Agent\": \"MiScraperEducativo/1.0 (+https://tusitio.com/contacto) - uso académico\"\n",
    "}\n",
    "\n",
    "# Realizamos la petición GET con los encabezados\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Comprobamos el código de estado HTTP\n",
    "if response.status_code == 200:\n",
    "    print(\"✅ Conexión exitosa. Página descargada correctamente.\\n\")\n",
    "    # Mostramos los primeros 500 caracteres del HTML\n",
    "    print(response.text[:500])\n",
    "else:\n",
    "    print(f\"⚠️ Error {response.status_code}: No se pudo acceder a la página.\")\n",
    "    print(response.text[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c9c5f-3d52-48b4-8298-a63935ab2b7b",
   "metadata": {},
   "source": [
    "A partir de aquí ya podríamos hacer scrapy. \n",
    "Pero antes hay que analizar **los aspectos legales y éticos** del webscraping y conocer las buenas prácticas:\n",
    "\n",
    "\n",
    "* Revisar el archivo robots.txt del sitio, generalmente está en **_https://sitio.com/robots.txt_** para ver qué partes permiten ser scrapeadas.\n",
    "\n",
    "* No hacer demasiadas peticiones seguidas (respetar el servidor). Muchas veces hasta te bloquea.\n",
    "\n",
    "* No recolectar información personal o protegida.\n",
    "\n",
    "* Usar siempre un User-Agent identificando al scraper.\n",
    "\n",
    "El objetivo es aprender y automatizar tareas sin dañar sitios ni infringir derechos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c86bd-7386-4643-8aa1-0dd68afd9fcf",
   "metadata": {},
   "source": [
    "### 1. BeautifulSoup\n",
    "\n",
    "**_BeautifulSoup_** es una librería de Python diseñada para analizar **_(parsear)_** documentos HTML o XML y extraer información específica de ellos.\n",
    "\n",
    "Podrías pensar en ella como una herramienta que convierte el HTML en una estructura de árbol, donde puedes moverte fácilmente por las etiquetas y obtener los datos que te interesan.\n",
    "\n",
    "Puede que necesitemos instalar **_pip install beautifulsoup4_**   y también requests **_pip install requests_**.\n",
    "\n",
    "Empecemos por un ejemplo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd2d550-b5d1-418a-ba72-7daafac5f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Código de estado: 200\n",
      "PERFECTO.... Ahora vamos a crear el objeto BeautifulSoup\n"
     ]
    }
   ],
   "source": [
    "## Paso 1.\n",
    "## Primero vamos a averiguar si la página responde correctamente, suele ser con el valor 200\n",
    "\n",
    "#importanciones y url diana\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://books.toscrape.com/\"\n",
    "\n",
    "#creamos un encabezado HTTP que identifica tu programa ante el servidor, indicando quién hace la petición (el “User-Agent”)\n",
    "# para evitar bloqueos y cumplir con las buenas prácticas de scraping.\n",
    "\n",
    "headers = {\"User-Agent\": \"MiScraperEducativo/1.0 (+https://tusitio.com/contacto)\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print(\"Código de estado:\", response.status_code)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"PERFECTO.... Ahora vamos a crear el objeto BeautifulSoup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4be063c-fe2e-4ad4-b4fb-84b6a0341240",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASO 2. Parsear\n",
    "## response.txt es una cadena de texto PLANO que tiene todo el código HTML que descargamos con requests\n",
    "## <!DOCTYPE html><html><head>...<body>...<div>...</div></body></html>\n",
    "## Beatifulsoup html.parser, Beatifulsoup toma ese texto y lo parsea creando el objeto DOM así conoces\n",
    "## los atributos, etiquetas y texo....\n",
    "## html.parser es la herramienta analizadora, y viene con Python. Hay otros como lxml o html5lib que son tolerantes a fallos\n",
    "## resumiendo sacamos el DOM de la página.\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679bc48-457d-4d4b-8518-4061bf5aaf6f",
   "metadata": {},
   "source": [
    "Ahora analizamos la estructura en este caso hay un bloque articel class=\"produc_pod\" y dentro hay\n",
    "\n",
    "* el título ``` ( <h3> <a title=\"...\"> )  ```\n",
    "* el precio ``` (<p  class=\"price_color\" > £51.77) ```\n",
    "\n",
    "Los buscamos y los almacenamos en un fichero .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c634ffc-32b2-44a0-afb9-5fb4078e014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datos guardados en 'libros.csv'\n"
     ]
    }
   ],
   "source": [
    "# PASO 3. Extraer datos.\n",
    "\n",
    "import csv\n",
    "\n",
    "libros = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "with open(\"libros.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Título\", \"Precio\"])\n",
    "\n",
    "    for libro in libros:\n",
    "        titulo = libro.h3.a['title']\n",
    "        precio = libro.find('p', class_='price_color').text\n",
    "        writer.writerow([titulo, precio])\n",
    "\n",
    "print(\" Datos guardados en 'libros.csv'\")\n",
    "\n",
    "# abrimos un fichero csv en modo excritura y vamos escribiendo con cada fila el título y el precio\n",
    "\n",
    "# Dentro del for estamos guardando dos variables título y precio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231bb6f-cc12-41f6-b1df-23fc1c524855",
   "metadata": {},
   "source": [
    "Que pasa si hay varias páginas, en este caso es de este formato, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd1da15-b037-47d8-8aae-6493dd315519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en 'libros2.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "#el formato de esta página es http://books.toscrape.com/catalogue/page-1.html\n",
    "# http://books.toscrape.com/catalogue/page-2.html\n",
    "#...\n",
    "\n",
    "url_base = \"http://books.toscrape.com/catalogue/page-{}.html\"  ##controlamos más de una página\n",
    "pagina = 1  #controla el número de página\n",
    "\n",
    "with open(\"libros2.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Título\", \"Precio\", \"Disponibilidad\"])\n",
    "\n",
    "    while True:\n",
    "        # el format es tomar la variable que tu especifues en ese string llaves\n",
    "        url = url_base.format(pagina)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            break  # No hay más páginas\n",
    "            \n",
    "        #en cada página necesitamos parsear \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        libros = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "        if not libros:\n",
    "            break  # Si no hay libros, terminamos\n",
    "\n",
    "        for libro in libros:\n",
    "            titulo = libro.h3.a['title']\n",
    "            precio = libro.find('p', class_='price_color').text\n",
    "            disponibilidad = libro.find('p', class_='instock availability').text.strip()  #strip elimina espacios\n",
    "            writer.writerow([titulo, precio, disponibilidad])\n",
    "\n",
    "        pagina += 1  #variable se suma a la siguiente\n",
    "\n",
    "print(\"Datos guardados en 'libros2.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972ec99-39fd-4b86-ae98-756e72e0b3db",
   "metadata": {},
   "source": [
    "Otra forma más robusta por si falla una página. Usa **_next_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55dc4284-ecf8-4e6f-bc9a-264680176ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en 'libros3.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"http://books.toscrape.com/catalogue/page-1.html\"  # Página inicial\n",
    "\n",
    "with open(\"libros3.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Título\", \"Precio\", \"Disponibilidad\"])\n",
    "\n",
    "    while url:  # Mientras haya una URL válida\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"No se pudo acceder a {url}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        libros = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "        for libro in libros:\n",
    "            titulo = libro.h3.a['title']\n",
    "            precio = libro.find('p', class_='price_color').text\n",
    "            disponibilidad = libro.find('p', class_='instock availability').text.strip()\n",
    "            writer.writerow([titulo, precio, disponibilidad])\n",
    "\n",
    "        # Buscar enlace \"next\"\n",
    "        next_page = soup.find('li', class_='next')\n",
    "        if next_page:\n",
    "            next_href = next_page.a['href']\n",
    "            # Construir la URL completa de la siguiente página\n",
    "            url = \"/\".join(url.split(\"/\")[:-1]) + \"/\" + next_href\n",
    "        else:\n",
    "            url = None  # No hay más páginas\n",
    "\n",
    "print(\"Datos guardados en 'libros3.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80e531-ff4e-4f87-9875-d9e3e200160c",
   "metadata": {},
   "source": [
    "Una herramienta de BeuatifulSoup es **_soup.prettify_** que nos permite ver mejor el DOM y es muy útil \n",
    "para analizar lo que queremos extreaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdddc3be-0927-43a2-aef6-234a1a4c32b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=\"text/css\"/>\n",
      "  <link href=\"../static/oscar/js/bootstrap-datetimepicker/bootstrap-datetimepicker.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"../static/oscar/css/datetimepicker.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      " </head>\n",
      " <body class=\"default\" id=\"default\">\n",
      "  <header class=\"header container-fluid\">\n",
      "   <div class=\"page_inner\">\n",
      "    <div class=\"row\">\n",
      "     <div class=\"col-sm-8 h1\">\n",
      "      <a href=\"../index.html\">\n",
      "       Books to Scrape\n",
      "      </a>\n",
      "      <small>\n",
      "       We love being scraped!\n",
      "      </small>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </header>\n",
      "  <div class=\"container-fluid page\">\n",
      "   <div class=\"page_inner\">\n",
      "    <ul class=\"breadcrumb\">\n",
      "     <li>\n",
      "      <a href=\"../index.html\">\n",
      "       Home\n",
      "      </a>\n",
      "     </li>\n",
      "     <li class=\"active\">\n",
      "      All products\n",
      "     </li>\n",
      "    </ul>\n",
      "    <div class=\"row\">\n",
      "     <aside class=\"sidebar col-sm-4 col-md-3\">\n",
      "      <div id=\"promotions_left\">\n",
      "      </div>\n",
      "      <div class=\"side_categories\">\n",
      "       <ul class=\"nav nav-list\">\n",
      "        <li>\n",
      "         <a href=\"cat\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Mostrar los primeros 1000 caracteres del DOM \"bonito\"\n",
    "print(soup.prettify()[1000:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abcca6-800a-4dcc-9d78-e55fc1167c4b",
   "metadata": {},
   "source": [
    "O si quieres mostrar solo los libros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4691f1-8474-44e4-94a4-6614b45321f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<article class=\"product_pod\">\n",
      " <div class=\"image_container\">\n",
      "  <a href=\"a-light-in-the-attic_1000/index.html\">\n",
      "   <img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"../media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/>\n",
      "  </a>\n",
      " </div>\n",
      " <p class=\"star-rating Three\">\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      " </p>\n",
      " <h3>\n",
      "  <a href=\"a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">\n",
      "   A Light in the ...\n",
      "  </a>\n",
      " </h3>\n",
      " <div class=\"product_price\">\n",
      "  <p class=\"price_color\">\n",
      "   Â£51.77\n",
      "  </p>\n",
      "  <p class=\"instock availability\">\n",
      "   <i class=\"icon-ok\">\n",
      "   </i>\n",
      "   In stock\n",
      "  </p>\n",
      "  <form>\n",
      "   <button class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\" type=\"submit\">\n",
      "    Add to basket\n",
      "   </button>\n",
      "  </form>\n",
      " </div>\n",
      "</article>\n",
      "\n",
      "<article class=\"product_pod\">\n",
      " <div class=\"image_container\">\n",
      "  <a href=\"tipping-the-velvet_999/index.html\">\n",
      "   <img alt=\"Tipping the Velvet\" class=\"thumbnail\" src=\"../media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg\"/>\n",
      "  </a>\n",
      " </div>\n",
      " <p class=\"star-rating One\">\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      "  <i class=\"icon-star\">\n",
      "  </i>\n",
      " </p>\n",
      " <h3>\n",
      "  <a href=\"tipping-the-velvet_999/index.html\" title=\"Tipping the Velvet\">\n",
      "   Tipping the Velvet\n",
      "  </a>\n",
      " </h3>\n",
      " <div class=\"product_price\">\n",
      "  <p class=\"price_color\">\n",
      "   Â£53.74\n",
      "  </p>\n",
      "  <p class=\"instock availability\">\n",
      "   <i class=\"icon-ok\">\n",
      "   </i>\n",
      "   In stock\n",
      "  </p>\n",
      "  <form>\n",
      "   <button class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\" type=\"submit\">\n",
      "    Add to basket\n",
      "   </button>\n",
      "  </form>\n",
      " </div>\n",
      "</article>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "libros = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "for libro in libros[:2]:  # Mostrar solo los primeros 3 libros para no saturar\n",
    "    print(libro.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c62738-11fb-4cb8-99b0-5844d43b8133",
   "metadata": {},
   "source": [
    "O en un navegador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3234e7f3-be7a-495a-91fb-9b94b2de9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pagina.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4be8c0-054c-4a9e-9608-9a86bfd91aba",
   "metadata": {},
   "source": [
    "### **Proyecto BeautifulSoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de5f7a-0751-44c9-a519-e2d6ad71b5f2",
   "metadata": {},
   "source": [
    "Debes elegir una página para scrapear con BeatifulSoup y guardar los datos en formato .csv, .json o el formato que desees pero es importante que tengas en cuenta que:\n",
    "\n",
    "* Sin login ni captchas: facilitan el scraping sin complicaciones.\n",
    "* Estructura HTML clara y consistente: que puedan localizar elementos de forma fácil\n",
    "* Páginas con paginación opcional, si es posible: para practicar whiles y extracción en varias páginas.\n",
    "* Atentos a algún aviso de tipo legal \n",
    "* No usar e-commerce como Amazon, Ebay, usan bloqueos, mucho JavaScript y suelen prohibir scraping.\n",
    "* Las redes sociales suelen dar problemas por temas de privacidad\n",
    "* Lo mejor son páginas de pequeños comercios\n",
    "\n",
    "\n",
    "Vuelca los datos en .json y aplícalo cuatro funciones de Pandas que no hayamos visto en clase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15bfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b337733-41c3-4147-8333-585a03f5a7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "035bddbd-d0db-48bf-a811-057b0fba4553",
   "metadata": {},
   "source": [
    "##  2. SCRAPY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3236d26-89a9-4805-af42-76712bb6294d",
   "metadata": {},
   "source": [
    "**_Scrapy_** es un framework de Python para el web scraping y el crawling. Permite extraer datos estructurados de sitios web, procesarlos y guardarlos fácilmente en distintos formatos (JSON, CSV, bases de datos...).\n",
    "\n",
    "A diferencia de *_BeautifulSoup_*, que es una librería para analizar HTML, Scrapy es un framework más completo que:\n",
    "\n",
    "* Gestiona las peticiones HTTP.\n",
    "\n",
    "* Controla la concurrencia (permite hacer muchas peticiones a la vez).\n",
    "\n",
    "* Sigue enlaces automáticamente.\n",
    "\n",
    "* Organiza el código en *_Spiders_*, *_Items_*, *_Pipelines_* y *_Middlewares_*.\n",
    "\n",
    "* Incluye herramientas de depuración, caché y exportación de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864943a1-1cde-4042-aaca-4fd9d0fc2788",
   "metadata": {},
   "source": [
    "Su arquitectura es la siguiente: *_Spider → Engine → Scheduler → Downloader → Middleware → Item Pipeline_*   \n",
    "\n",
    "**Spider** \n",
    "\n",
    "* Es el corazón del scraping.\n",
    "\n",
    "* Define de dónde se parte (URL inicial) y cómo se extrae la información.\n",
    "\n",
    "* Cada spider es una clase Python que hereda de scrapy.Spider.\n",
    "\n",
    "**_Engine_**\n",
    "\n",
    "* El motor **controla el flujo interno** entre los distintos componentes:\n",
    "\n",
    "* Envía las peticiones al Scheduler.\n",
    "\n",
    "* Recibe las respuestas del Downloader.\n",
    "\n",
    "* Entrega los datos al Spider.\n",
    "\n",
    "* Manda los resultados al Pipeline.\n",
    "\n",
    "* No se programa directamente, pero es esencial para entender cómo circula la información.\n",
    "\n",
    "**_Scheduler (Planificador)_**\n",
    "\n",
    "* Mantiene una **cola de peticiones** pendientes.\n",
    "\n",
    "* Decide qué URL visitar a continuación.\n",
    "\n",
    "* Evita visitar la misma página más de una vez (usa un fingerprint).\n",
    "\n",
    "**_Downloader (Descargador)_**\n",
    "\n",
    "* Se encarga de hacer las **peticiones HTTP** reales.\n",
    "\n",
    "* Usa middlewares para modificar cabeceras, retrasos, cookies, etc.\n",
    "\n",
    "* También puede simular user agents o manejar proxies.\n",
    "\n",
    "**_Middlewares_**\n",
    "\n",
    "* Son capas intermedias que permiten **interceptar y modificar** peticiones o respuestas. Ejemplo: cambiar el User-Agent, usar un proxy, manejar cookies, etc.\n",
    "\n",
    "**_Item Pipeline_**\n",
    "\n",
    "* Se encarga del **postprocesamiento** de los datos. Puede:\n",
    "\n",
    "* Limpiar o validar campos.\n",
    "\n",
    "* Guardar los datos en ficheros JSON/CSV.\n",
    "\n",
    "* Insertar en bases de datos (MongoDB, MySQL...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e47f88-c793-4186-ae5e-b40fa2046807",
   "metadata": {},
   "source": [
    "**Componentes clave de un proyecto Scrapy**\n",
    "\n",
    "* *_spiders/_*\t:       Carpeta con los spiders que definen qué y cómo se extrae.\n",
    "* *_items.py_*\t:       Define la estructura de los datos extraídos.\n",
    "* *_pipelines.py_*\t:   Procesa y guarda los datos.\n",
    "* *_middlewares.py_* :  Controla cómo se hacen las peticiones.\n",
    "* *_settings.py_*\t:   Configura el proyecto (user-agent, delays, pipelines, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68625a-894e-48e8-80ef-0bbe57b1b5a6",
   "metadata": {},
   "source": [
    "Los comandos básicos son\n",
    "\n",
    "Instalación, por si no está instlado\n",
    "\n",
    "**_pip install scrapy_**        \n",
    "\n",
    "Creación del proyecto, suele ponerse como nombre del proyecto algo relacionado con la web datoscoches, datosarticulos....\n",
    "\n",
    "**_scrapy startproject nombreproyecto_**   \n",
    "\n",
    "Ejecución y recogida de datos\n",
    "\n",
    "**_scrapy crawl nombre_spider -o  resultados.json_**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc0f7c-d73c-4994-8f66-3660ead0bbb6",
   "metadata": {},
   "source": [
    "Resumiendo. Ciclo de Vida. Todo es asíncrono, así ningún paso bloque al otro, gracias a la herramienta Twisted un framework Python que permite gestionar particiones.\n",
    "\n",
    "1. El Spider genera las primeras peticiones (start_requests o start_urls).\n",
    "\n",
    "2. El Engine envía las peticiones al Scheduler, que las almacena.\n",
    "\n",
    "3. El Engine toma una petición del Scheduler y la pasa al Downloader.\n",
    "\n",
    "4. El Downloader ejecuta la solicitud HTTP (mediante Twisted).\n",
    "\n",
    "5. Cuando llega la respuesta, pasa por los Downloader Middlewares.\n",
    "\n",
    "6. El Engine entrega la respuesta al método parse() del Spider.\n",
    "\n",
    "7. El Spider genera Items o nuevas Requests.\n",
    "\n",
    "8. Los Items se envían al Item Pipeline, las Requests vuelven al Scheduler.\n",
    "\n",
    "9. El ciclo continúa hasta que no quedan peticiones activas.\n",
    "\n",
    "\n",
    "Esta es la **estructura básica** de un proyecto cuando lo creamos:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cd5e30d-69b8-4e37-93d4-4c3d47e2d96c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "nombre_proyecto/\n",
    "│\n",
    "├── nombre_proyecto/\n",
    "│   ├── spiders/         → aquí van los spiders\n",
    "│   ├── items.py         → estructura de datos\n",
    "│   ├── pipelines.py     → limpieza/almacenamiento\n",
    "│   ├── middlewares.py   → configuración de peticiones\n",
    "│   ├── settings.py      → ajustes generales\n",
    "│   └── __init__.py\n",
    "│\n",
    "└── scrapy.cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51363a56-b864-47b2-ac04-3cfc9e3e59f2",
   "metadata": {},
   "source": [
    "Supuesto práctico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e33fea-8cd7-4b09-ae33-5377518ea48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'citas_scraper', using template directory '/opt/conda/lib/python3.11/site-packages/scrapy/templates/project', created in:\n",
      "    /home/jovyan/work/UD1/citas_scraper\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd citas_scraper\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "#Desde un terminal o un bloque de Jupyterlab con ! delante permite ejecutar comandos\n",
    "!scrapy startproject citas_scraper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f73fcd0-ca4f-4db0-97c5-5bf130a6a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/UD1/citas_scraper\n"
     ]
    }
   ],
   "source": [
    "#entramos en citas !  y % son comando mágicos\n",
    "\n",
    "%cd citas_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1754df7-95b8-424d-9a19-a170b8fc7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'citas' using template 'basic' in module:\n",
      "  citas_scraper.spiders.citas\n"
     ]
    }
   ],
   "source": [
    "# creamos un spider, fíjate le pasamos la dirección\n",
    "\n",
    "!scrapy genspider citas quotes.toscrape.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a3270-444c-4685-9c4c-12d1a4873a9c",
   "metadata": {},
   "source": [
    "Abrimos citas desde el arbol de directorios izquierda.\n",
    "Se abre en un nueva ventana y añadimos esto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356eb46-4d07-43d0-b5e7-52c601a76e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class CitasSpider(scrapy.Spider):\n",
    "    name = \"citas\"  #fíjate es como llamaremos al spider\n",
    "    allowed_domains = [\"quotes.toscrape.com\"]\n",
    "    start_urls = [\"https://quotes.toscrape.com\"]\n",
    "    # es el equivalente del this, pero le pasa todo realmetne la clase \n",
    "    def parse(self, response): #ek response es la sopa \"\"\n",
    "            # Recorre todas las citas del bloque <div class=\"quote\">\n",
    "        for cita in response.css('div.quote'):\n",
    "            yield {\n",
    "                'texto': cita.css('span.text::text').get(),\n",
    "                'autor': cita.css('small.author::text').get(),\n",
    "                'tags': cita.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "\n",
    "        # Busca el enlace a la siguiente página\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "\n",
    "        # Si existe, lo sigue\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbbb2c-356b-4284-a48e-0d90abe71bb5",
   "metadata": {},
   "source": [
    "Ya solo nos queda ejecutar. Verás que var recorriendo las diferentes páginas cargando las citas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603088e-9ae9-4632-a7b1-753e5125b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#> log.txt 2>&1  para que no muestre pantalla solo guarde, si lo quitas lo muestra en pantalla\n",
    "\n",
    "!scrapy crawl citas -o resultados.json  > log.txt 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e385c-37b1-4141-9e69-875781b0b44f",
   "metadata": {},
   "source": [
    "Ya tenemos en un fichero json, pero ¿y si queremos mostrar en pandas? Recuerda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba798732-46cd-4044-947b-2facb46731ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/UD1/citas_scraper\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "autor\n",
       "Albert Einstein           10\n",
       "J.K. Rowling               9\n",
       "Marilyn Monroe             7\n",
       "Dr. Seuss                  6\n",
       "Mark Twain                 6\n",
       "C.S. Lewis                 5\n",
       "Jane Austen                5\n",
       "Bob Marley                 3\n",
       "Eleanor Roosevelt          2\n",
       "Charles Bukowski           2\n",
       "Suzanne Collins            2\n",
       "George R.R. Martin         2\n",
       "Ralph Waldo Emerson        2\n",
       "Mother Teresa              2\n",
       "Ernest Hemingway           2\n",
       "J.D. Salinger              1\n",
       "George Bernard Shaw        1\n",
       "J.R.R. Tolkien             1\n",
       "Alfred Tennyson            1\n",
       "Terry Pratchett            1\n",
       "John Lennon                1\n",
       "George Carlin              1\n",
       "W.C. Fields                1\n",
       "Ayn Rand                   1\n",
       "Jimi Hendrix               1\n",
       "J.M. Barrie                1\n",
       "E.E. Cummings              1\n",
       "Khaled Hosseini            1\n",
       "Harper Lee                 1\n",
       "Helen Keller               1\n",
       "Haruki Murakami            1\n",
       "Stephenie Meyer            1\n",
       "Garrison Keillor           1\n",
       "Thomas A. Edison           1\n",
       "Douglas Adams              1\n",
       "Elie Wiesel                1\n",
       "Friedrich Nietzsche        1\n",
       "André Gide                 1\n",
       "Allen Saunders             1\n",
       "Pablo Neruda               1\n",
       "Jim Henson                 1\n",
       "Alexandre Dumas fils       1\n",
       "Charles M. Schulz          1\n",
       "William Nicholson          1\n",
       "Jorge Luis Borges          1\n",
       "George Eliot               1\n",
       "Martin Luther King Jr.     1\n",
       "James Baldwin              1\n",
       "Steve Martin               1\n",
       "Madeleine L'Engle          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%cd /home/jovyan/work/UD1/citas_scraper\n",
    " \n",
    "# Leer el fichero JSON generado por Scrapy\n",
    "df = pd.read_json(\"resultados.json\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "df.head()\n",
    "\n",
    "len(df)  #cuantas citas\n",
    "\n",
    "df['autor'].unique()  #listar autores\n",
    "\n",
    "df['autor'].value_counts()  #citas por autor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89577e04-1331-4755-adbb-be1915f5c0aa",
   "metadata": {},
   "source": [
    "### **Proyecto Scrapy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec777a40-2d2d-4dba-8852-d599bc3e1172",
   "metadata": {},
   "source": [
    "Existen algunas webs que permiten trabajar con scrapy. \n",
    "Es muy importante webs que tengan un **HTML muy limpio**, y en lo posible básico, y selectores CSS muy claros. \n",
    "Suelen ser supermercados, inmobiliarias, ventas de coches de ocasión pero no de grandes corpopraciones. \n",
    "Sobre todo **evitar javascript**\n",
    "\n",
    "Desarrollar un spider Scrapy que recorra un catálogo de productos de un sitio web público y extraiga los siguientes datos, como mínimo:\n",
    "\n",
    "Nombre o título del producto.\n",
    "\n",
    "Precio.\n",
    "\n",
    "Estado o disponibilidad. (si es posible)\n",
    "\n",
    "Categoría o etiqueta (si existe).\n",
    "\n",
    "Cualquier otro campo que consideres oportuno. \n",
    "\n",
    "El spider debe ser capaz de recorrer varias páginas mediante la paginación del sitio.\n",
    "\n",
    "Los datos extraídos se almacenarán en un fichero .json o .csv para su posterior análisis en pandas.\n",
    "En este punto investiga 4 o 5 funciones de Pandas que no hayamos visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713cbc7-7962-49e0-bd3e-7b564ff7e1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
